{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import trajectories\n",
    "import noctiluca as nl\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some fBms\n",
    "\n",
    "## fBm generator functions\n",
    "\n",
    "Define some useful functions for generating the fBms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fgn(fbm_generator, sample_dimension, n_time, n_sample, file_path):\n",
    "    \"\"\"A helper function for generating fractional Gaussian noise samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fbm_generator : trajectories.FractionalBrownianMotion\n",
    "        The FractionalBrownianMotion object for simulating the noise samples\n",
    "    sample_dimension : int\n",
    "        The spatial dimension for the samples (in this case 3)\n",
    "    n_time : int\n",
    "        The time dimension for the samples/number of time points desired.\n",
    "    n_sample : int\n",
    "        The number of trajectories to simulate.\n",
    "    file_path : str\n",
    "        The output path to save the .npy\n",
    "    \"\"\"\n",
    "    fgn = fbm_generator.computation_method(fbm_generator.covariance_sequence,\n",
    "                                           n_time,\n",
    "                                           (sample_dimension, n_sample))\n",
    "    np.save(file_path, fgn)\n",
    "\n",
    "def generate_multiple_fgns(fbm_generator, sample_dimension, n_time, n_sample,\n",
    "                           output_path, max_sample_size=1):\n",
    "    \"\"\"A helper function for generating multiple fgn samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fbm_generator : trajectories.FractionalBrownianMotion\n",
    "        The FractionalBrownianMotion object for simulating the noise samples\n",
    "    sample_dimension : int\n",
    "        The spatial dimension for the samples (in this case 3)\n",
    "    n_time : int\n",
    "        The time dimension for the samples/number of time points desired.\n",
    "    n_sample : int\n",
    "        The number of trajectories to simulate.\n",
    "    output_path : str\n",
    "        The output path to save the .npy files\n",
    "    max_sample_size : int, optional\n",
    "        The maximum number of samples to generate at once, by default 1\n",
    "    \"\"\"\n",
    "    sample_sizes = (n_sample // max_sample_size) * [max_sample_size] + \\\n",
    "        [n_sample % max_sample_size]\n",
    "    file_prefix = f\"fgn_hurst_{fbm_generator.hurst}_kr_{fbm_generator.k_r}_n_time_{n_time}\"\n",
    "    for i, sample_size in enumerate(sample_sizes):\n",
    "        if sample_size > 0:\n",
    "            generate_fgn(fbm_generator, sample_dimension, n_time, sample_size,\n",
    "                         os.path.join(output_path, file_prefix + f\"_{i}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fGns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_exponents = [0.2, 0.3, 0.4, 0.5, 0.6] # exponents used for simulation\n",
    "\n",
    "# a generator object for each exponent. Note that the covariance sequences for\n",
    "# each generator will be cached to speed up computation.\n",
    "fbm_generators = [trajectories.FractionalBrownianMotion(1, me / 2)\n",
    "                   for me in msd_exponents]\n",
    "sample_dimension = 3 # spatial dimension\n",
    "n_time = 100000000 # number of time points\n",
    "n_sample = 1000 # number of samples to generate\n",
    "max_sample_size = 1 # maximum number of trajectories per simulation\n",
    "\n",
    "output_path = \"\" # where to save the outputs\n",
    "n_processes = 1 # number of threads to dedicate to multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fbm_generator in tqdm(fbm_generators):\n",
    "    sample_sizes = (n_sample // max_sample_size) * [max_sample_size] + [n_sample % max_sample_size]\n",
    "    file_prefix = f\"fgn_hurst_{str(fbm_generator.hurst).replace(\".\", \"p\")}_kr_{str(fbm_generator.k_r).replace(\".\", \"p\")}_n_time_{n_time}\"\n",
    "\n",
    "    def parfun(i_sample_size):\n",
    "        i, sample_size = i_sample_size\n",
    "        if sample_size > 0:\n",
    "            generate_fgn(fbm_generator, sample_dimension, n_time, sample_size,\n",
    "                         os.path.join(output_path, file_prefix + f\"_{i}\"))\n",
    "\n",
    "    with Pool(processes=n_processes) as mypool:\n",
    "        mypool.map(parfun, list(enumerate(sample_sizes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine some MSDs\n",
    "Let's take a look at some of the MSDs to confirm they look like how we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_fgn_paths = [\"\"] # provide list of paths to files you would like to examine\n",
    "msd_fbms = []\n",
    "for msd_fgn_path in msd_fgn_paths:\n",
    "    fgn = np.load(msd_fgn_path)\n",
    "    fbm = np.cumsum(fgn, axis=0)\n",
    "    fbm -= fbm[0, :, :]\n",
    "    msd_fbms.append(fbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['orangered', 'orchid', 'orange', 'gray', 'royalblue']\n",
    "msd_exponents = [] # a list of the MSD exponent of each selected file\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "for color, exponent, fbm in zip(colors, msd_exponents, msd_fbms):\n",
    "    data = nl.TaggedSet()\n",
    "    fbm_split = np.split(fbm, 10000)\n",
    "    for i in range(10):\n",
    "\n",
    "        data.add(nl.Trajectory(np.squeeze(fbm_split[i])))\n",
    "\n",
    "    artists = nl.plot.msd_overview(data, dt=1, label=f\"{exponent}\")\n",
    "\n",
    "    for a in artists[:-1]:\n",
    "        a.remove()\n",
    "    artists[-1].set_color(color)\n",
    "\n",
    "plt.xlabel(\"frame\")\n",
    "plt.ylabel(\"displacement\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate first passage times\n",
    "\n",
    "Here, we used a series of target distances from the origin combined with a \n",
    "series of target sizes to determine conditions for which the observed FPTs \n",
    "appeared to converge. \n",
    "\n",
    "## Define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fpts(path, target_distances, target_sizes, output_path):\n",
    "    \"\"\"A helper function for calculating first passage times for a given\n",
    "    trajectory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        A path to a fgn sample as generated above\n",
    "    target_distances : list | np.array\n",
    "        An iterable of target distances from the origin\n",
    "    target_sizes : list | np.array\n",
    "        An iterable of target sizes\n",
    "    output_path : str\n",
    "        A path in which to save the intermediate fpt calculations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The calculated fpts\n",
    "    \"\"\"\n",
    "    path_split = path.split(\"/\")[-1].split(\"_\")\n",
    "    hurst, traj_idx = path_split[2], path_split[-1].split(\".\")[0]\n",
    "\n",
    "    fgn = np.load(path)\n",
    "    fbm = np.cumsum(fgn, axis=0)\n",
    "    fbm -= fbm[0, :, :]\n",
    "\n",
    "    fpts = []\n",
    "\n",
    "    for target_distance in target_distances:\n",
    "        target_position = np.array([target_distance, 0, 0]).reshape((1, 3, 1))\n",
    "\n",
    "        # calculate distances to target position for pts in fbm\n",
    "        distances_to_target = np.linalg.norm(fbm - target_position, axis=1)\n",
    "        for target_size in target_sizes:\n",
    "            if target_size >= target_distance:\n",
    "                fpt = 0\n",
    "            else:\n",
    "                fpt = np.sum(np.cumsum(distances_to_target < target_size, axis=0) == 0)\n",
    "            fpts.append([target_distance, target_size, fpt,\n",
    "                         float(hurst.split(\"p\")[-1])/10, int(traj_idx)])\n",
    "\n",
    "    out = np.array(fpts)\n",
    "    np.save(os.path.join(output_path,\n",
    "                         path.split(\"/\")[-1]), out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate FPTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgn_lists = [] # provide a list of file paths to fgns\n",
    "\n",
    "# define target distances and target sizes\n",
    "target_distances = np.sqrt(3) * np.logspace(1, 3.5, num=15, base=2)\n",
    "target_sizes = np.sqrt(3) * np.logspace(0, np.log2(5), num=8, base=2)\n",
    "\n",
    "# output path\n",
    "output_path = \"\"\n",
    "\n",
    "# specify number of cores\n",
    "n_processes = 1\n",
    "\n",
    "def parfun(fgn_path):\n",
    "    return calculate_fpts(fgn_path, target_distances, target_sizes,\n",
    "                          output_path)\n",
    "\n",
    "todo = fgn_lists\n",
    "with Pool(processes=n_processes) as mypool:\n",
    "    fpt_list = list(tqdm(mypool.imap(parfun, todo), total=len(todo)))\n",
    "\n",
    "all_fpts = np.concatenate(fpt_list)\n",
    "\n",
    "# specify desired save location\n",
    "np.save(os.path.join(\"\", \"all_first_passage_times.npy\"), all_fpts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot survival distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpt_df = []\n",
    "for fpt_file_path in os.listdir(output_path):\n",
    "    fpt_df.append(np.load(os.path.join(output_path, fpt_file_path)))\n",
    "\n",
    "fpt_df = np.concatenate(fpt_df)\n",
    "fpt_df = pd.DataFrame(fpt_df, columns=[\"target_distance\", \"target_size\", \"fpt\", \"alpha\", \"index\"])\n",
    "\n",
    "# correct some naminging\n",
    "fpt_df.loc[fpt_df[\"alpha\"] == 1.5, \"alpha\"] = 0.15\n",
    "fpt_df.loc[fpt_df[\"alpha\"] == 2.5, \"alpha\"] = 0.25\n",
    "\n",
    "# convert H to alpha\n",
    "fpt_df[\"alpha\"] *= 2\n",
    "fpt_df[\"status\"] = True\n",
    "fpt_df.loc[fpt_df[\"fpt\"] == 100000000.0, \"status\"] = False\n",
    "fpt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.unique(fpt_df[\"alpha\"])\n",
    "survival_curves = {}\n",
    "\n",
    "for i, ts_td in tqdm(enumerate(product(target_sizes, target_distances))):\n",
    "    ts, td = ts_td\n",
    "    curr_df = fpt_df[fpt_df[\"target_size\"] == ts]\n",
    "    curr_df = curr_df[curr_df[\"target_distance\"] == td]\n",
    "    for alpha in alphas:\n",
    "        time_km, survival_prob, conf_int = kaplan_meier_estimator(\n",
    "                curr_df.loc[curr_df[\"alpha\"] == alpha, \"status\"],\n",
    "            curr_df.loc[curr_df[\"alpha\"] == alpha, \"fpt\"], conf_type=\"log-log\"\n",
    "            )\n",
    "\n",
    "        survival_curves[(td, ts, alpha)] = [time_km, survival_prob, conf_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(alphas),\n",
    "                        figsize=(16, 4), sharey=True, sharex=True)\n",
    "\n",
    "tstds = sorted([ts / td for ts, td in product(target_sizes, target_distances)])\n",
    "tstds_dict = {tstd : i for i, tstd in enumerate(tstds)}\n",
    "\n",
    "# colors = plt.cm.viridis(np.linspace(min(tstds), max(tstds), len(tstds)))\n",
    "sorted_ts_td = sorted(list(product(target_sizes, target_distances)), key=lambda tup: tup[0] / tup[1])\n",
    "\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    for ts, td in reversed(sorted_ts_td):\n",
    "\n",
    "        time_km, survival_prob, conf_int = survival_curves.get((td, ts, alpha), [[], [], []])\n",
    "\n",
    "\n",
    "        rescaled_time = time_km / (td**(2 / alpha))\n",
    "\n",
    "        axs[i].step(rescaled_time, survival_prob, where=\"post\",\n",
    "                                     label=f\"{ts:.2f} / {td:.2f}\",\n",
    "                   color=plt.cm.viridis(ts/td))\n",
    "\n",
    "    axs[i].set_ylabel(r\"$\\hat{S}(t)$\")\n",
    "    axs[i].set_xlabel(r\"$t / x^{2 / \\alpha}$\")\n",
    "    axs[i].set_title(f\"Alpha {alpha}\")\n",
    "    axs[i].set_xscale(\"log\")\n",
    "    axs[i].set_yscale(\"log\")\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=plt.cm.viridis), ax=axs[i], label=\"$r / x$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict based on target size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(alphas),\n",
    "                        figsize=(16, 4), sharey=True, sharex=True)\n",
    "\n",
    "# restrict target size\n",
    "tstds = sorted([ts / td for ts, td in product(target_sizes[target_sizes > 3], target_distances)])\n",
    "tstds_dict = {tstd : i for i, tstd in enumerate(tstds)}\n",
    "\n",
    "sorted_ts_td = sorted(list(product(target_sizes[target_sizes > 3], target_distances)), key=lambda tup: tup[0] / tup[1])\n",
    "\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    lineplots = []\n",
    "    for ts, td in reversed(sorted_ts_td):\n",
    "\n",
    "        time_km, survival_prob, conf_int = survival_curves.get((td, ts, alpha), [[], [], []])\n",
    "\n",
    "        rescaled_time = time_km / (td**(2 / alpha))\n",
    "\n",
    "        axs[i].step(rescaled_time, survival_prob, where=\"post\",\n",
    "                                     label=f\"{ts:.2f} / {td:.2f}\",\n",
    "                   color=plt.cm.viridis(ts/td))\n",
    "\n",
    "    axs[i].set_ylabel(r\"$\\hat{S}(t)$\")\n",
    "    axs[i].set_xlabel(r\"$t / x^{2 / \\alpha}$\")\n",
    "    axs[i].set_title(f\"Alpha {alpha}\")\n",
    "    axs[i].set_xscale(\"log\")\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=plt.cm.viridis), ax=axs[i], label=\"$r / x$\")\n",
    "\n",
    "axs[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize \"converged\" distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(alphas),\n",
    "                        figsize=(16, 4), sharey=True, sharex=True)\n",
    "\n",
    "# These ones are visually converged\n",
    "tstds = np.array(sorted([ts / td for ts, td in product(target_sizes[target_sizes > 3], target_distances)]))\n",
    "tstds = tstds[tstds < 0.25]\n",
    "tstds_dict = {tstd : i for i, tstd in enumerate(tstds)}\n",
    "\n",
    "sorted_ts_td = sorted(list(product(target_sizes, target_distances)), key=lambda tup: tup[0] / tup[1])\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    lineplots = []\n",
    "    for ts, td in reversed(sorted_ts_td):\n",
    "        if ts / td in tstds:\n",
    "            time_km, survival_prob, conf_int = survival_curves.get((td, ts, alpha), [[], [], []])\n",
    "\n",
    "            rescaled_time = time_km / (td**(2 / alpha))\n",
    "\n",
    "            axs[i].step(rescaled_time, survival_prob, where=\"post\",\n",
    "                                         label=f\"{ts:.2f} / {td:.2f}\",\n",
    "                       color=plt.cm.viridis(ts/td))\n",
    "\n",
    "    axs[i].set_ylabel(r\"$\\hat{S}(t)$\")\n",
    "    axs[i].set_xlabel(r\"$t / x^{2 / \\alpha}$\")\n",
    "    axs[i].set_title(f\"Alpha {alpha}\")\n",
    "    axs[i].set_xscale(\"log\")\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=plt.cm.viridis), ax=axs[i], label=\"$r / x$\")\n",
    "\n",
    "axs[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale converged distributions by median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, figsize=(6, 4), sharey=True, sharex=True)\n",
    "\n",
    "tstds = np.array(sorted([ts / td for ts, td in product(target_sizes[target_sizes > 3], target_distances)]))\n",
    "tstds = tstds[tstds < 0.25]\n",
    "tstds_dict = {tstd : i for i, tstd in enumerate(tstds)}\n",
    "\n",
    "sorted_ts_td = sorted(list(product(target_sizes, target_distances)), key=lambda tup: tup[0] / tup[1])\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    curr_time, curr_survival_prob = [], []\n",
    "    for ts, td in reversed(sorted_ts_td):\n",
    "        if ts / td in tstds:\n",
    "            time_km, survival_prob, conf_int = survival_curves.get((td, ts, alpha), [[], [], []])\n",
    "\n",
    "            med_idx = np.argmin(np.abs(survival_prob-0.5))\n",
    "            rescaled_time = time_km / (td**(2 / alpha))\n",
    "            rescaled_time /= rescaled_time[med_idx]\n",
    "            curr_time += list(rescaled_time)\n",
    "            curr_survival_prob += list(survival_prob)\n",
    "\n",
    "            axs.step(rescaled_time, survival_prob, where=\"post\",\n",
    "                       label=fr\"$\\alpha$ = {alpha}\", color = plt.cm.tab10(i))\n",
    "\n",
    "axs.set_ylabel(r\"$\\hat{S}(t)$\")\n",
    "axs.set_xlabel(r\"$t / x^{2 / \\alpha}$\")\n",
    "axs.set_xscale(\"log\")\n",
    "axs.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "# https://stackoverflow.com/questions/13588920/stop-matplotlib-repeating-labels-in-legend\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "plt.title(\"First passage time survival curves\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling values to produce desired intersection\n",
    "\n",
    "rescaling_values = {0.2: 4.11522633744856e-05,\n",
    "                    0.3: 0.011919622032168262,\n",
    "                    0.4: 0.20286020648339484,\n",
    "                    0.5: 1.1111111111111112,\n",
    "                    0.6: 3.452480562170953}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_grid = np.logspace(-1, 5, num = 25)\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=(9, 5), sharey=True, sharex=True)\n",
    "\n",
    "tstds = np.array(sorted([ts / td for ts, td in product(target_sizes[target_sizes > 3], target_distances)]))\n",
    "tstds = tstds[tstds < 0.25]\n",
    "tstds_dict = {tstd : i for i, tstd in enumerate(tstds)}\n",
    "\n",
    "sorted_ts_td = sorted(list(product(target_sizes, target_distances)), key=lambda tup: tup[0] / tup[1])\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    median_fpts = []\n",
    "    curr_curve_grid = []\n",
    "    for ts, td in reversed(sorted_ts_td):\n",
    "        if ts / td in tstds:\n",
    "            time_km, survival_prob, conf_int = survival_curves.get((td, ts, alpha), [[], [], []])\n",
    "\n",
    "            rescaled_time = time_km / (td**(2 / alpha))\n",
    "            rescaled_time /= rescaling_values[alpha]\n",
    "            med_idx = np.argmin(np.abs(survival_prob-0.5))\n",
    "            for x in curve_grid:\n",
    "                curr_curve_grid.append(x)\n",
    "                curve_grid_times = (x ** 2 / 2648)**(1 / alpha) * rescaled_time\n",
    "                median_fpts.append(curve_grid_times[med_idx])\n",
    "\n",
    "\n",
    "    axs.plot(curr_curve_grid, median_fpts, label=fr\"$\\alpha = {alpha}$\")\n",
    "\n",
    "axs.set_ylabel(\"Median FPT (s)\")\n",
    "axs.set_xlabel(\"Target distance (nm)\")\n",
    "axs.set_xscale(\"log\")\n",
    "axs.set_yscale(\"log\")\n",
    "\n",
    "axs.vlines(1e4, 10e-35, 10e34, \"k\", linestyle=\"dotted\")\n",
    "axs.text(1.1e4, 1e-7, \"size of a nucleus\\n(~10um)\")\n",
    "\n",
    "axs.vlines(10, 10e-35, 10e34, \"k\", linestyle=\"dotted\")\n",
    "axs.text(1.1e1, 1e-14, \"diameter of a nucleosome\\n(~10nm)\")\n",
    "\n",
    "axs.hlines(86400, 10e-2, 10e6, \"k\", linestyle=\"dotted\")\n",
    "axs.text(1.2e1, 3e5, \"length of a \\ncell cycle (~24h)\")\n",
    "\n",
    "axs.hlines(0.01, 10e-2, 10e6, \"k\", linestyle=\"dotted\")\n",
    "axs.text(3e2, 5e-2, \"time for Pol II to transcribe\\none nucleotide (~10ms)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim([0.5, 1e5])\n",
    "plt.ylim([10e-16, 10e10])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
